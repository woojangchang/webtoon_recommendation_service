{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ec942-228f-49e5-8962-a22c14916b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "nwb = pd.read_csv('data/NAVER.csv', encoding='cp949', usecols=['title', 'genre', 'story', 'platform'])\n",
    "print(nwb.shape)\n",
    "nwb.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1790a0cc-47e5-49a7-b411-dbfd6b003962",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb_end = pd.read_csv('data/Naver_FINISH.csv', encoding='cp949', usecols=['title', 'genre', 'story', 'platform'])\n",
    "print(nwb_end.shape)\n",
    "nwb_end.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7df978-dbbb-4305-bc3d-97dd602ef466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연재 중, 완결 웹툰 합치기\n",
    "nwb = pd.concat([nwb, nwb_end])\n",
    "nwb.reset_index(inplace=True, drop=True)\n",
    "nwb.fillna('완결', inplace=True)\n",
    "print(nwb.shape)\n",
    "nwb.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba2bc0d-5b13-471c-9f86-fa390b8212d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lz = pd.read_csv('data/LZ.csv', encoding='cp949', usecols=['title', 'genre', 'story', 'platform'])\n",
    "print(lz.shape)\n",
    "lz.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b4768-9449-495f-aaa1-a455fbbbed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버, 레진 웹툰 합치기\n",
    "wb = pd.concat([nwb, lz])\n",
    "wb.reset_index(inplace=True, drop=True)\n",
    "print(wb.shape)\n",
    "wb.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233c82b-284c-49a0-ae9e-2ba0a01819df",
   "metadata": {},
   "source": [
    "## 전처리\n",
    "- 장르, 줄거리에 대하여 특수문자, 숫자 등을 없앤 순수 한글만 남김\n",
    "- 제목도 하려 했으나, 토큰화 할 때 오류가 생겨 하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d66bcc4-1ad0-4893-9a11-8f5268d5ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def hangul(text):\n",
    "    return re.sub('[^ㄱ-ㅎ|ㅏ-ㅣ|가-힣]', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5591815c-7b65-4845-bc4c-938d0ff9bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_copy = wb.copy()\n",
    "wb_copy['genre'] = wb_copy['genre'].apply(lambda x:hangul(x))\n",
    "wb_copy['story'] = wb_copy['story'].apply(lambda x:hangul(x))\n",
    "\n",
    "wb_copy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e53e16-c01a-4897-ad75-c5048ee3a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wb['story'][11])\n",
    "print(wb_copy['story'][11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7aa7ea-26b0-495d-ae0c-62e44ffe4c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stopwords_korean.txt', 'r', encoding='utf8') as f:\n",
    "    stopwords = f.readline()\n",
    "stopwords = stopwords.split()\n",
    "\n",
    "stopwords += ['이야기', '시작', '날', '보다', '이다']\n",
    "print(stopwords[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f75838-8096-4781-87e4-8742ca770ece",
   "metadata": {},
   "source": [
    "## 토큰화\n",
    "- KoNLPy 설치 (교재 538 ~ 540 페이지)\n",
    "- Mecab 설치(윈도우) (https://cleancode-ws.tistory.com/97)\n",
    "- stemming : 갇힌 > 갇히다 등 단어의 원형으로 되돌리는 것\n",
    "    - konlpy에서는 Okt만 지원함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e790a-e253-4d22-a258-6e85b8f06f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt() # Okt : Open Korean Text\n",
    "\n",
    "def okt_tokenizer(text):\n",
    "    malist = okt.pos(text, norm=True, stem=True)\n",
    "    filtered_words = []\n",
    "    # 필요한 어구만 대상으로 하기\n",
    "    for word in malist:\n",
    "        # 어미/조사/구두점 제외\n",
    "        if not word[1] in ['Josa', 'Eomi', 'Punctuation']:\n",
    "            if (word[0] not in stopwords) and (len(word[0]) > 1):\n",
    "                filtered_words.append(word[0])\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee419db-2161-4751-8fe0-a493a7438f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(okt_tokenizer(wb_copy['story'][11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fee969-f67d-4ed2-8ddd-9bb33a9a1691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e3dfc-01bf-4d3e-a6ed-7cd7bdb420a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "font_path = \"C:/Windows/Fonts/NGULIM.TTF\"\n",
    "font = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ed2e24-5d93-41fa-9e0a-852fbb0c21e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#텍스트 데이터를 리스트로 변환\n",
    "Data_list=wb_copy.story.values.tolist()\n",
    "\n",
    "#리스트를 요소별로 가져와서 토큰화 후 저장\n",
    "data_word=[]\n",
    "for i in range(len(Data_list)):\n",
    "    data_word.append(okt_tokenizer(Data_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e402c3-3476-48bd-a419-7e67aed4a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "for words in data_word[:3]:\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255f60dc-a295-495c-be3f-84e808ba7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_word)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_word\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f039f-4246-43f8-99e8-3a8cfd0277a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        print(num_topics, end=' ')\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                              id2word=id2word,\n",
    "                                              num_topics=num_topics, \n",
    "                                              random_state=100,\n",
    "                                              update_every=1,\n",
    "                                              chunksize=100,\n",
    "                                              passes=10,\n",
    "                                              alpha='auto',\n",
    "                                              per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616920a6-ca0a-458b-b077-972b155ef8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=texts, start=3, limit=21, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6def1d56-fc36-43ab-ad0f-8fa2ac6420b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=21; start=3; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4161534b-ecb0-4e97-90ec-45ced55549c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fff7003-f3ec-4b59-9a6a-aa472c475b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Select the model and print the topics\n",
    "optimal_model = model_list[8]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e9c0c-a335-4279-87c4-1958d927fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(optimal_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d40fab8-8889-4962-bbb4-3aaa95ed2563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Multi",
   "language": "python",
   "name": "multi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
